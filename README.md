<h1 align="center">ğŸš€ DisCO: Reinforcing Large Reasoning Models with Discriminative Constrained Optimization</h1>

The success of **DeepSeek-R1** has spotlighted **GRPO (Group Relative Policy Optimization)** as a key reinforcement learning method for large reasoning models.

**But how can we improve GRPO? What are its key limitations?**

We analyzed GRPO under a binary reward setting and uncovered two core insights:

* âš ï¸ GRPO suffers from **question-level difficulty bias**
* ğŸ” GRPO has a surprising connection to **discriminative learning** techniques, particularly AUC maximization

---

### ğŸ’¡ Introducing **DisCO** â€” *Discriminative Constrained Optimization*

**DisCO** is a new RL framework grounded in **discriminative learning**. It trains models by **increasing scores for positive answers while decreasing those for negatives**, enabling:

* âš¡ Faster convergence
* ğŸ”’ More stable optimization
* ğŸ” Longer-lasting training dynamics for large reasoning models

---

### ğŸ” Why DisCO?

* âŒ **No more difficulty bias** â€“ replaces group-relative rewards with discriminative scoring
* ğŸ”„ **No clip operations** â€“ uses non-clipping scoring functions for smoother learning
* ğŸ“‰ **Stable training** â€“ via simple constrained optimization to keep KL divergence in check
* âš–ï¸ **Handles sparse rewards** â€“ robust to imbalanced data with more negatives than positives

---

### ğŸ“ˆ Results

On six math reasoning benchmarks with a 1.5B model, **DisCO outperforms GRPO and its variants**:

* **+7% vs GRPO**
* **+6% vs DAPO**

**Table of Contents**
- [Experimental Results](#experimental-results)
- [Getting Started](#getting-started)
    - [Environment Setup](#environment-setup)
    - [Training](#training)
    - [Evaluation](#evaluation)
- [Citing DRRho](#citing-drrho)


