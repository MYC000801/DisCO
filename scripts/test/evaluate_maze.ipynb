{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5835caff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# pip install -U pandas pyarrow\n",
    "df = pd.read_parquet(\"/projectnb/rlhf/mingyuc/DisCO/datasets/maze/test.parquet\")   # 若是 S3 路径同样支持\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "650298c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_list = df['extra_info'].apply(lambda x: x['chat']).tolist( )  # 提取 'chat' 字段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ffb8d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-23 00:04:36 config.py:1670] Downcasting torch.float32 to torch.float16.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 07-23 00:04:40 arg_utils.py:953] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\n",
      "INFO 07-23 00:04:40 config.py:1005] Chunked prefill is enabled with max_num_batched_tokens=512.\n",
      "INFO 07-23 00:04:40 llm_engine.py:237] Initializing an LLM engine (vdev) with config: model='/projectnb/rlhf/mingyuc/DisCO/exp/qwen_1.5b/global_step_20', speculative_config=None, tokenizer='/projectnb/rlhf/mingyuc/DisCO/exp/qwen_1.5b/global_step_20', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/projectnb/rlhf/mingyuc/DisCO/exp/qwen_1.5b/global_step_20, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=True multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "INFO 07-23 00:04:41 model_runner.py:1060] Starting to load model /projectnb/rlhf/mingyuc/DisCO/exp/qwen_1.5b/global_step_20...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.28s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:05<00:00,  3.29s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:05<00:00,  2.99s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-23 00:04:47 model_runner.py:1071] Loading model weights took 2.9104 GB\n",
      "INFO 07-23 00:04:47 gpu_executor.py:122] # GPU blocks: 82897, # CPU blocks: 9362\n",
      "INFO 07-23 00:04:47 gpu_executor.py:126] Maximum concurrency for 131072 tokens per request: 10.12x\n",
      "INFO 07-23 00:04:50 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 07-23 00:04:50 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 07-23 00:04:59 model_runner.py:1530] Graph capturing finished in 10 secs.\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "#del llm\n",
    "\n",
    "# 替换为你的本地模型路径\n",
    "local_model_path = \"/projectnb/rlhf/mingyuc/DisCO/exp/qwen_1.5b/global_step_20\"\n",
    "\n",
    "#local_model_path = \"/projectnb/rlhf/mingyuc/DisCO/exp/qwen1.5/global_step_78\"\n",
    "\n",
    "#local_model_path = \"Qwen/Qwen2.5-1.5B\"\n",
    "\n",
    "llm = LLM(model=local_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6dea9d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 21.87it/s, est. speed input: 3090.14 toks/s, output: 131.45 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 3):\n",
      "[{'content': '(7, 2): path, (7, 4): path, (6, 3): path, (8, 3): wall', 'role': 'user'}\n",
      " {'content': '(6, 3)', 'role': 'assistant'}]\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sampling_params = SamplingParams(temperature=0.6, max_tokens=6)\n",
    "tokenizer = llm.get_tokenizer()\n",
    "\n",
    "index = 1\n",
    "\n",
    "\n",
    "chat_exp = chat_list[3]\n",
    "print(len(chat_exp))\n",
    "test_chat = chat_exp[:index]\n",
    "chat_str = tokenizer.apply_chat_template(test_chat, add_generation_prompt=True, tokenize=False)\n",
    "chat_str = chat_str.replace(\n",
    "    \"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\",\n",
    "    \"<|im_start|>system\\nYou are an intelligent agent navigating a maze.\\nAt each step, you receive an observation of four adjacent cells, described by their coordinates and whether they are 'path' or 'wall'.\\nYou must choose exactly one adjacent cell that is a valid 'path' or 'exit' and move into it.\\nAlways move efficiently toward the goal.\\nOutput your next move as a single coordinate in the format (row, col).\\nDo not explain or repeat the input — just return the next move.\\n<|im_end|>\"\n",
    ")\n",
    "output = llm.generate(chat_str, sampling_params)\n",
    "print(output[0].outputs[0].text)\n",
    "print(chat_exp[index-1:index+1])\n",
    "\n",
    "result = chat_exp[index]['content'] in output[0].outputs[0].text\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c00ae95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 3412/3412 [01:29<00:00, 38.11it/s, est. speed input: 40298.81 toks/s, output: 228.66 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总数: 3412, 正确数: 2950, 正确率: 0.8646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "sampling_params = SamplingParams(temperature=0.0, max_tokens=6)\n",
    "tokenizer = llm.get_tokenizer()\n",
    "chat_strs = []\n",
    "targets = []\n",
    "\n",
    "for chat_exp in chat_list[:100]:\n",
    "    for index in range(1, len(chat_exp), 2):\n",
    "        test_chat = chat_exp[:index]\n",
    "        chat_str = tokenizer.apply_chat_template(test_chat, add_generation_prompt=True, tokenize=False)\n",
    "        chat_str = chat_str.replace(\n",
    "            \"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\",\n",
    "            \"<|im_start|>system\\nYou are an intelligent agent navigating a maze.\\nAt each step, you receive an observation of four adjacent cells, described by their coordinates and whether they are 'path' or 'wall'.\\nYou must choose exactly one adjacent cell that is a valid 'path' or 'exit' and move into it.\\nAlways move efficiently toward the goal.\\nOutput your next move as a single coordinate in the format (row, col).\\nDo not explain or repeat the input — just return the next move.\\n<|im_end|>\"\n",
    "        )\n",
    "        chat_strs.append(chat_str)\n",
    "        targets.append(chat_exp[index]['content'])\n",
    "\n",
    "# 并行推理\n",
    "outputs = llm.generate(chat_strs, sampling_params)\n",
    "\n",
    "# 统计正确率\n",
    "total = len(outputs)\n",
    "correct = 0\n",
    "for i, output in enumerate(outputs):\n",
    "    result = targets[i] in output.outputs[0].text\n",
    "    if result:\n",
    "        correct += 1\n",
    "\n",
    "accuracy = correct / total if total > 0 else 0\n",
    "print(f\"总数: {total}, 正确数: {correct}, 正确率: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6128b9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(temperature=0.0, max_tokens=6)\n",
    "tokenizer = llm.get_tokenizer()\n",
    "chat_strs = []\n",
    "targets = []\n",
    "\n",
    "for chat_exp in chat_list[:100]:\n",
    "    for index in range(1, len(chat_exp), 2):\n",
    "        test_chat = chat_exp[:index]\n",
    "        chat_str = tokenizer.apply_chat_template(test_chat, add_generation_prompt=True, tokenize=False)\n",
    "        chat_str = chat_str.replace(\n",
    "            \"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\",\n",
    "            \"<|im_start|>system\\nYou are an intelligent agent navigating a maze.\\nAt each step, you receive an observation of four adjacent cells, described by their coordinates and whether they are 'path' or 'wall'.\\nYou must choose exactly one adjacent cell that is a valid 'path' or 'exit' and move into it.\\nAlways move efficiently toward the goal.\\nOutput your next move as a single coordinate in the format (row, col).\\nDo not explain or repeat the input — just return the next move.\\n<|im_end|>\"\n",
    "        )\n",
    "        chat_strs.append(chat_str)\n",
    "        targets.append(chat_exp[index]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb929047",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = []\n",
    "for chat_exp in chat_list[0:3000]:\n",
    "    for index in range(1, len(chat_exp), 2):\n",
    "        test_chat = chat_exp[:index]\n",
    "        answer = chat_exp[index]['content']\n",
    "        data.append({\n",
    "            \"extra_info\": {\n",
    "                \"question\": test_chat,\n",
    "                \"answer\": answer\n",
    "            }\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f14412f",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import Dataset\n",
    "df.to_parquet('/projectnb/rlhf/mingyuc/DisCO/datasets/maze/test_new.parquet',\n",
    "              engine='pyarrow',           # 推荐，用 Arrow 写更快\n",
    "              index=False)                # 不把行索引写进去"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f228bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "apo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
