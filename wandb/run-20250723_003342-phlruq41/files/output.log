Using LocalLogger is deprecated. The constructor API will change
Total training steps: 129
/projectnb/replearn/mingyu/anaconda/envs/disco/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]
step:0 - train/loss:1.414 - train/lr(1e-3):0.001
step:1 - train/loss:1.436 - train/lr(1e-3):0.002
step:2 - train/loss:1.431 - train/lr(1e-3):0.003
step:3 - train/loss:1.345 - train/lr(1e-3):0.003
step:4 - train/loss:1.062 - train/lr(1e-3):0.004
step:5 - train/loss:0.400 - train/lr(1e-3):0.005
step:6 - train/loss:0.319 - train/lr(1e-3):0.006
step:7 - train/loss:0.228 - train/lr(1e-3):0.007
step:8 - train/loss:0.202 - train/lr(1e-3):0.008
step:9 - train/loss:0.177 - train/lr(1e-3):0.008
step:10 - train/loss:0.141 - train/lr(1e-3):0.009
step:11 - train/loss:0.095 - train/lr(1e-3):0.010
step:12 - train/loss:0.093 - train/lr(1e-3):0.010
step:13 - train/loss:0.088 - train/lr(1e-3):0.010
step:14 - train/loss:0.072 - train/lr(1e-3):0.010
step:15 - train/loss:0.052 - train/lr(1e-3):0.010
step:16 - train/loss:0.051 - train/lr(1e-3):0.010
step:17 - train/loss:0.047 - train/lr(1e-3):0.010
step:18 - train/loss:0.067 - train/lr(1e-3):0.010
step:19 - train/loss:0.045 - train/lr(1e-3):0.010
step:20 - train/loss:0.040 - train/lr(1e-3):0.010
step:21 - train/loss:0.058 - train/lr(1e-3):0.010
step:22 - train/loss:0.038 - train/lr(1e-3):0.010
step:23 - train/loss:0.044 - train/lr(1e-3):0.010
step:24 - train/loss:0.050 - train/lr(1e-3):0.010
step:25 - train/loss:0.050 - train/lr(1e-3):0.010
step:26 - train/loss:0.042 - train/lr(1e-3):0.010
step:27 - train/loss:0.040 - train/lr(1e-3):0.010
step:28 - train/loss:0.043 - train/lr(1e-3):0.009
step:29 - train/loss:0.043 - train/lr(1e-3):0.009
step:30 - train/loss:0.034 - train/lr(1e-3):0.009
step:31 - train/loss:0.039 - train/lr(1e-3):0.009
step:32 - train/loss:0.033 - train/lr(1e-3):0.009
step:33 - train/loss:0.039 - train/lr(1e-3):0.009
step:34 - train/loss:0.031 - train/lr(1e-3):0.009
step:35 - train/loss:0.029 - train/lr(1e-3):0.009
step:36 - train/loss:0.044 - train/lr(1e-3):0.009
step:37 - train/loss:0.042 - train/lr(1e-3):0.009
step:38 - train/loss:0.039 - train/lr(1e-3):0.009
step:39 - train/loss:0.032 - train/lr(1e-3):0.009
step:40 - train/loss:0.027 - train/lr(1e-3):0.009
step:41 - train/loss:0.024 - train/lr(1e-3):0.008
step:42 - train/loss:0.039 - train/lr(1e-3):0.008
step:43 - train/loss:0.033 - train/lr(1e-3):0.008
step:44 - train/loss:0.052 - train/lr(1e-3):0.008
step:45 - train/loss:0.032 - train/lr(1e-3):0.008
step:46 - train/loss:0.026 - train/lr(1e-3):0.008
step:47 - train/loss:0.027 - train/lr(1e-3):0.008
step:48 - train/loss:0.034 - train/lr(1e-3):0.008
step:49 - train/loss:0.037 - train/lr(1e-3):0.008
step:50 - train/loss:0.023 - train/lr(1e-3):0.008
step:51 - train/loss:0.031 - train/lr(1e-3):0.007
step:52 - train/loss:0.033 - train/lr(1e-3):0.007
step:53 - train/loss:0.026 - train/lr(1e-3):0.007
step:54 - train/loss:0.037 - train/lr(1e-3):0.007
step:55 - train/loss:0.032 - train/lr(1e-3):0.007
step:56 - train/loss:0.034 - train/lr(1e-3):0.007
step:57 - train/loss:0.035 - train/lr(1e-3):0.007
step:58 - train/loss:0.029 - train/lr(1e-3):0.007
step:59 - train/loss:0.030 - train/lr(1e-3):0.006
step:60 - train/loss:0.027 - train/lr(1e-3):0.006
step:61 - train/loss:0.032 - train/lr(1e-3):0.006
step:62 - train/loss:0.030 - train/lr(1e-3):0.006
step:63 - train/loss:0.038 - train/lr(1e-3):0.006
step:64 - train/loss:0.025 - train/lr(1e-3):0.006
step:65 - train/loss:0.020 - train/lr(1e-3):0.006
step:66 - train/loss:0.028 - train/lr(1e-3):0.005
step:67 - train/loss:0.028 - train/lr(1e-3):0.005
step:68 - train/loss:0.031 - train/lr(1e-3):0.005
step:69 - train/loss:0.031 - train/lr(1e-3):0.005
step:70 - train/loss:0.022 - train/lr(1e-3):0.005
step:71 - train/loss:0.023 - train/lr(1e-3):0.005
step:72 - train/loss:0.032 - train/lr(1e-3):0.005
step:73 - train/loss:0.025 - train/lr(1e-3):0.005
step:74 - train/loss:0.026 - train/lr(1e-3):0.004
step:75 - train/loss:0.024 - train/lr(1e-3):0.004
step:76 - train/loss:0.033 - train/lr(1e-3):0.004
step:77 - train/loss:0.027 - train/lr(1e-3):0.004
step:78 - train/loss:0.027 - train/lr(1e-3):0.004
step:79 - train/loss:0.021 - train/lr(1e-3):0.004
step:80 - train/loss:0.022 - train/lr(1e-3):0.004
step:81 - train/loss:0.036 - train/lr(1e-3):0.003
step:82 - train/loss:0.029 - train/lr(1e-3):0.003
step:83 - train/loss:0.023 - train/lr(1e-3):0.003
step:84 - train/loss:0.017 - train/lr(1e-3):0.003
step:85 - train/loss:0.033 - train/lr(1e-3):0.003
step:86 - train/loss:0.022 - train/lr(1e-3):0.003
step:87 - train/loss:0.022 - train/lr(1e-3):0.003
step:88 - train/loss:0.026 - train/lr(1e-3):0.003
step:89 - train/loss:0.033 - train/lr(1e-3):0.003
step:90 - train/loss:0.028 - train/lr(1e-3):0.002
step:91 - train/loss:0.019 - train/lr(1e-3):0.002
step:92 - train/loss:0.022 - train/lr(1e-3):0.002
step:93 - train/loss:0.025 - train/lr(1e-3):0.002
step:94 - train/loss:0.031 - train/lr(1e-3):0.002
step:95 - train/loss:0.022 - train/lr(1e-3):0.002
step:96 - train/loss:0.025 - train/lr(1e-3):0.002
step:97 - train/loss:0.030 - train/lr(1e-3):0.002
step:98 - train/loss:0.030 - train/lr(1e-3):0.002
step:99 - train/loss:0.021 - train/lr(1e-3):0.001
step:100 - train/loss:0.021 - train/lr(1e-3):0.001
step:101 - train/loss:0.028 - train/lr(1e-3):0.001
step:102 - train/loss:0.025 - train/lr(1e-3):0.001
step:103 - train/loss:0.021 - train/lr(1e-3):0.001
step:104 - train/loss:0.025 - train/lr(1e-3):0.001
step:105 - train/loss:0.019 - train/lr(1e-3):0.001
step:106 - train/loss:0.025 - train/lr(1e-3):0.001
step:107 - train/loss:0.028 - train/lr(1e-3):0.001
step:108 - train/loss:0.023 - train/lr(1e-3):0.001
step:109 - train/loss:0.023 - train/lr(1e-3):0.001
step:110 - train/loss:0.028 - train/lr(1e-3):0.001
step:111 - train/loss:0.025 - train/lr(1e-3):0.001
step:112 - train/loss:0.022 - train/lr(1e-3):0.000
step:113 - train/loss:0.025 - train/lr(1e-3):0.000
step:114 - train/loss:0.022 - train/lr(1e-3):0.000
step:115 - train/loss:0.023 - train/lr(1e-3):0.000
step:116 - train/loss:0.025 - train/lr(1e-3):0.000
step:117 - train/loss:0.019 - train/lr(1e-3):0.000
step:118 - train/loss:0.028 - train/lr(1e-3):0.000
step:119 - train/loss:0.024 - train/lr(1e-3):0.000
step:120 - train/loss:0.030 - train/lr(1e-3):0.000
step:121 - train/loss:0.024 - train/lr(1e-3):0.000
step:122 - train/loss:0.035 - train/lr(1e-3):0.000
step:123 - train/loss:0.021 - train/lr(1e-3):0.000
step:124 - train/loss:0.026 - train/lr(1e-3):0.000
step:125 - train/loss:0.027 - train/lr(1e-3):0.000
step:126 - train/loss:0.017 - train/lr(1e-3):0.000
step:127 - train/loss:0.024 - train/lr(1e-3):0.000
step:128 - train/loss:0.022 - train/lr(1e-3):0.000
step:129 - val/loss:0.023
/projectnb/replearn/mingyu/anaconda/envs/disco/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
