Validation: Generation end.
("Initial validation metrics: {'val/test_score/lighteval/MATH': "
 '0.5125549278091651}')
Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): [36mray::WorkerDict.actor_rollout_generate_sequences()[39m (pid=714834, ip=192.168.21.190, actor_id=955d1538a655b275d6ee752a01000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x1495d2125660>)
  File "/projectnb/rlhf/mingyuc/DisCO/verl/verl/third_party/vllm/vllm_v_0_6_3/llm.py", line 151, in init_cache_engine
    self.llm_engine.init_cache_engine()
  File "/projectnb/rlhf/mingyuc/DisCO/verl/verl/third_party/vllm/vllm_v_0_6_3/llm_engine_sp.py", line 347, in init_cache_engine
    self.model_executor.init_cache_engine()
  File "/projectnb/rlhf/mingyuc/DisCO/verl/verl/third_party/vllm/vllm_v_0_6_3/spmd_gpu_executor.py", line 157, in init_cache_engine
    self.worker._init_cache_engine()
  File "/projectnb/rlhf/mingyuc/DisCO/verl/verl/third_party/vllm/vllm_v_0_6_3/worker.py", line 237, in _init_cache_engine
    super()._init_cache_engine()
  File "/projectnb/replearn/mingyu/anaconda/envs/disco/lib/python3.10/site-packages/vllm/worker/worker.py", line 275, in _init_cache_engine
    self.cache_engine = [
  File "/projectnb/replearn/mingyu/anaconda/envs/disco/lib/python3.10/site-packages/vllm/worker/worker.py", line 276, in <listcomp>
    CacheEngine(self.cache_config, self.model_config,
  File "/projectnb/replearn/mingyu/anaconda/envs/disco/lib/python3.10/site-packages/vllm/worker/cache_engine.py", line 63, in __init__
    self.gpu_cache = self._allocate_kv_cache(
  File "/projectnb/replearn/mingyu/anaconda/envs/disco/lib/python3.10/site-packages/vllm/worker/cache_engine.py", line 82, in _allocate_kv_cache
    torch.zeros(kv_cache_shape,
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 802.00 MiB. GPU 0 has a total capacity of 44.40 GiB of which 786.31 MiB is free. Including non-PyTorch memory, this process has 43.62 GiB memory in use. Of the allocated memory 41.81 GiB is allocated by PyTorch, and 929.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

During handling of the above exception, another exception occurred:

[36mray::WorkerDict.actor_rollout_generate_sequences()[39m (pid=714834, ip=192.168.21.190, actor_id=955d1538a655b275d6ee752a01000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x1495d2125660>)
  File "/projectnb/rlhf/mingyuc/DisCO/verl/verl/single_controller/ray/base.py", line 399, in func
    return getattr(self.worker_dict[key], name)(*args, **kwargs)
  File "/projectnb/rlhf/mingyuc/DisCO/verl/verl/single_controller/base/decorator.py", line 404, in inner
    return func(*args, **kwargs)
  File "/projectnb/rlhf/mingyuc/DisCO/verl/verl/workers/fsdp_workers.py", line 445, in generate_sequences
    output = self.rollout.generate_sequences(prompts=prompts)
  File "/projectnb/replearn/mingyu/anaconda/envs/disco/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/projectnb/rlhf/mingyuc/DisCO/verl/verl/workers/rollout/vllm_rollout/vllm_rollout.py", line 280, in generate_sequences
    self.inference_engine = LLM(
  File "/projectnb/rlhf/mingyuc/DisCO/verl/verl/third_party/vllm/vllm_v_0_6_3/llm.py", line 147, in __init__
    self.llm_engine = LLMEngine.from_engine_args(model, tokenizer, engine_args)  # TODO: check usagecontext
  File "/projectnb/rlhf/mingyuc/DisCO/verl/verl/third_party/vllm/vllm_v_0_6_3/llm_engine_sp.py", line 393, in from_engine_args
    engine = cls(
  File "/projectnb/rlhf/mingyuc/DisCO/verl/verl/third_party/vllm/vllm_v_0_6_3/llm_engine_sp.py", line 212, in __init__
    self.model_executor = executor_class(
  File "/projectnb/rlhf/mingyuc/DisCO/verl/verl/third_party/vllm/vllm_v_0_6_3/spmd_gpu_executor.py", line 71, in __init__
    self._init_executor(model, distributed_init_method)
  File "/projectnb/rlhf/mingyuc/DisCO/verl/verl/third_party/vllm/vllm_v_0_6_3/spmd_gpu_executor.py", line 78, in _init_executor
    self._init_workers_sp(model, distributed_init_method)
  File "/projectnb/rlhf/mingyuc/DisCO/verl/verl/third_party/vllm/vllm_v_0_6_3/spmd_gpu_executor.py", line 112, in _init_workers_sp
    self.worker.load_model()
  File "/projectnb/replearn/mingyu/anaconda/envs/disco/lib/python3.10/site-packages/vllm/worker/worker.py", line 183, in load_model
    self.model_runner.load_model()
  File "/projectnb/rlhf/mingyuc/DisCO/verl/verl/third_party/vllm/vllm_v_0_6_3/model_runner.py", line 104, in load_model
    self.model = get_model(
  File "/projectnb/rlhf/mingyuc/DisCO/verl/verl/third_party/vllm/vllm_v_0_6_3/model_loader.py", line 45, in get_model
    return loader.load_model(
  File "/projectnb/rlhf/mingyuc/DisCO/verl/verl/third_party/vllm/vllm_v_0_6_3/model_loader.py", line 128, in load_model
    model = _initialize_model(model_config, self.load_config, lora_config, cache_config, scheduler_config)
  File "/projectnb/replearn/mingyu/anaconda/envs/disco/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 175, in _initialize_model
    return build_model(
  File "/projectnb/replearn/mingyu/anaconda/envs/disco/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 160, in build_model
    return model_class(config=hf_config,
  File "/projectnb/replearn/mingyu/anaconda/envs/disco/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py", line 393, in __init__
    self.model = Qwen2Model(config, cache_config, quant_config)
  File "/projectnb/replearn/mingyu/anaconda/envs/disco/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py", line 248, in __init__
    self.start_layer, self.end_layer, self.layers = make_layers(
  File "/projectnb/replearn/mingyu/anaconda/envs/disco/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 407, in make_layers
    [PPMissingLayer() for _ in range(start_layer)] + [
  File "/projectnb/replearn/mingyu/anaconda/envs/disco/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 408, in <listcomp>
    maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
  File "/projectnb/replearn/mingyu/anaconda/envs/disco/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py", line 250, in <lambda>
    lambda prefix: Qwen2DecoderLayer(config=config,
  File "/projectnb/replearn/mingyu/anaconda/envs/disco/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py", line 184, in __init__
    self.mlp = Qwen2MLP(
  File "/projectnb/replearn/mingyu/anaconda/envs/disco/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py", line 65, in __init__
    self.gate_up_proj = MergedColumnParallelLinear(
  File "/projectnb/replearn/mingyu/anaconda/envs/disco/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 424, in __init__
    super().__init__(input_size=input_size,
  File "/projectnb/replearn/mingyu/anaconda/envs/disco/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 304, in __init__
    self.quant_method.create_weights(
  File "/projectnb/replearn/mingyu/anaconda/envs/disco/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 122, in create_weights
    weight = Parameter(torch.empty(sum(output_partition_sizes),
  File "/projectnb/replearn/mingyu/anaconda/envs/disco/lib/python3.10/site-packages/torch/utils/_device.py", line 79, in __torch_function__
    return func(*args, **kwargs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 86.00 MiB. GPU 0 has a total capacity of 44.40 GiB of which 20.31 MiB is free. Including non-PyTorch memory, this process has 44.37 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 493.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): [36mray::WorkerDict.actor_rollout_generate_sequences()[39m (pid=714833, ip=192.168.21.190, actor_id=b0a541115caf408b37f3c89501000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x151a8a13d6f0>)
  File "/projectnb/rlhf/mingyuc/DisCO/verl/verl/third_party/vllm/vllm_v_0_6_3/llm.py", line 151, in init_cache_engine
    self.llm_engine.init_cache_engine()
  File "/projectnb/rlhf/mingyuc/DisCO/verl/verl/third_party/vllm/vllm_v_0_6_3/llm_engine_sp.py", line 347, in init_cache_engine
    self.model_executor.init_cache_engine()
  File "/projectnb/rlhf/mingyuc/DisCO/verl/verl/third_party/vllm/vllm_v_0_6_3/spmd_gpu_executor.py", line 157, in init_cache_engine
    self.worker._init_cache_engine()
  File "/projectnb/rlhf/mingyuc/DisCO/verl/verl/third_party/vllm/vllm_v_0_6_3/worker.py", line 237, in _init_cache_engine
    super()._init_cache_engine()
  File "/projectnb/replearn/mingyu/anaconda/envs/disco/lib/python3.10/site-packages/vllm/worker/worker.py", line 275, in _init_cache_engine
    self.cache_engine = [
  File "/projectnb/replearn/mingyu/anaconda/envs/disco/lib/python3.10/site-packages/vllm/worker/worker.py", line 276, in <listcomp>
    CacheEngine(self.cache_config, self.model_config,
  File "/projectnb/replearn/mingyu/anaconda/envs/disco/lib/python3.10/site-packages/vllm/worker/cache_engine.py", line 63, in __init__
    self.gpu_cache = self._allocate_kv_cache(
  File "/projectnb/replearn/mingyu/anaconda/envs/disco/lib/python3.10/site-packages/vllm/worker/cache_engine.py", line 82, in _allocate_kv_cache
    torch.zeros(kv_cache_shape,
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 802.00 MiB. GPU 0 has a total capacity of 44.40 GiB of which 534.31 MiB is free. Including non-PyTorch memory, this process has 43.87 GiB memory in use. Of the allocated memory 41.81 GiB is allocated by PyTorch, and 1.15 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

During handling of the above exception, another exception occurred:

[36mray::WorkerDict.actor_rollout_generate_sequences()[39m (pid=714833, ip=192.168.21.190, actor_id=b0a541115caf408b37f3c89501000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x151a8a13d6f0>)
  File "/projectnb/rlhf/mingyuc/DisCO/verl/verl/single_controller/ray/base.py", line 399, in func
    return getattr(self.worker_dict[key], name)(*args, **kwargs)
  File "/projectnb/rlhf/mingyuc/DisCO/verl/verl/single_controller/base/decorator.py", line 404, in inner
    return func(*args, **kwargs)
  File "/projectnb/rlhf/mingyuc/DisCO/verl/verl/workers/fsdp_workers.py", line 445, in generate_sequences
    output = self.rollout.generate_sequences(prompts=prompts)
  File "/projectnb/replearn/mingyu/anaconda/envs/disco/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/projectnb/rlhf/mingyuc/DisCO/verl/verl/workers/rollout/vllm_rollout/vllm_rollout.py", line 280, in generate_sequences
    self.inference_engine = LLM(
  File "/projectnb/rlhf/mingyuc/DisCO/verl/verl/third_party/vllm/vllm_v_0_6_3/llm.py", line 147, in __init__
    self.llm_engine = LLMEngine.from_engine_args(model, tokenizer, engine_args)  # TODO: check usagecontext
  File "/projectnb/rlhf/mingyuc/DisCO/verl/verl/third_party/vllm/vllm_v_0_6_3/llm_engine_sp.py", line 393, in from_engine_args
    engine = cls(
  File "/projectnb/rlhf/mingyuc/DisCO/verl/verl/third_party/vllm/vllm_v_0_6_3/llm_engine_sp.py", line 212, in __init__
    self.model_executor = executor_class(
  File "/projectnb/rlhf/mingyuc/DisCO/verl/verl/third_party/vllm/vllm_v_0_6_3/spmd_gpu_executor.py", line 71, in __init__
    self._init_executor(model, distributed_init_method)
  File "/projectnb/rlhf/mingyuc/DisCO/verl/verl/third_party/vllm/vllm_v_0_6_3/spmd_gpu_executor.py", line 78, in _init_executor
    self._init_workers_sp(model, distributed_init_method)
  File "/projectnb/rlhf/mingyuc/DisCO/verl/verl/third_party/vllm/vllm_v_0_6_3/spmd_gpu_executor.py", line 112, in _init_workers_sp
    self.worker.load_model()
  File "/projectnb/replearn/mingyu/anaconda/envs/disco/lib/python3.10/site-packages/vllm/worker/worker.py", line 183, in load_model
    self.model_runner.load_model()
  File "/projectnb/rlhf/mingyuc/DisCO/verl/verl/third_party/vllm/vllm_v_0_6_3/model_runner.py", line 104, in load_model
    self.model = get_model(
  File "/projectnb/rlhf/mingyuc/DisCO/verl/verl/third_party/vllm/vllm_v_0_6_3/model_loader.py", line 45, in get_model
    return loader.load_model(
  File "/projectnb/rlhf/mingyuc/DisCO/verl/verl/third_party/vllm/vllm_v_0_6_3/model_loader.py", line 128, in load_model
    model = _initialize_model(model_config, self.load_config, lora_config, cache_config, scheduler_config)
  File "/projectnb/replearn/mingyu/anaconda/envs/disco/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 175, in _initialize_model
    return build_model(
  File "/projectnb/replearn/mingyu/anaconda/envs/disco/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 160, in build_model
    return model_class(config=hf_config,
  File "/projectnb/replearn/mingyu/anaconda/envs/disco/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py", line 393, in __init__
    self.model = Qwen2Model(config, cache_config, quant_config)
  File "/projectnb/replearn/mingyu/anaconda/envs/disco/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py", line 240, in __init__
    self.embed_tokens = VocabParallelEmbedding(
  File "/projectnb/replearn/mingyu/anaconda/envs/disco/lib/python3.10/site-packages/vllm/model_executor/layers/vocab_parallel_embedding.py", line 260, in __init__
    self.linear_method.create_weights(self,
  File "/projectnb/replearn/mingyu/anaconda/envs/disco/lib/python3.10/site-packages/vllm/model_executor/layers/vocab_parallel_embedding.py", line 28, in create_weights
    weight = Parameter(torch.empty(sum(output_partition_sizes),
  File "/projectnb/replearn/mingyu/anaconda/envs/disco/lib/python3.10/site-packages/torch/utils/_device.py", line 79, in __torch_function__
    return func(*args, **kwargs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 594.00 MiB. GPU 0 has a total capacity of 44.40 GiB of which 534.31 MiB is free. Including non-PyTorch memory, this process has 43.87 GiB memory in use. Of the allocated memory 41.81 GiB is allocated by PyTorch, and 1.15 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): [36mray::WorkerDict.actor_rollout_generate_sequences()[39m (pid=714520, ip=192.168.21.190, actor_id=b203ed3be4c45c7e69acd4da01000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x14b7e0078160>)
  File "/projectnb/replearn/mingyu/anaconda/envs/disco/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 1665, in execute_model
    hidden_or_intermediate_states = model_executable(
  File "/projectnb/replearn/mingyu/anaconda/envs/disco/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/projectnb/replearn/mingyu/anaconda/envs/disco/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/projectnb/replearn/mingyu/anaconda/envs/disco/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py", line 415, in forward
    hidden_states = self.model(input_ids, positions, kv_caches,
  File "/projectnb/replearn/mingyu/anaconda/envs/disco/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/projectnb/replearn/mingyu/anaconda/envs/disco/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/projectnb/replearn/mingyu/anaconda/envs/disco/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py", line 288, in forward
    hidden_states, residual = layer(
  File "/projectnb/replearn/mingyu/anaconda/envs/disco/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/projectnb/replearn/mingyu/anaconda/envs/disco/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/projectnb/replearn/mingyu/anaconda/envs/disco/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py", line 220, in forward
    hidden_states = self.mlp(hidden_states)
  File "/projectnb/replearn/mingyu/anaconda/envs/disco/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/projectnb/replearn/mingyu/anaconda/envs/disco/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/projectnb/replearn/mingyu/anaconda/envs/disco/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py", line 79, in forward
    gate_up, _ = self.gate_up_proj(x)
  File "/projectnb/replearn/mingyu/anaconda/envs/disco/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/projectnb/replearn/mingyu/anaconda/envs/disco/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/projectnb/replearn/mingyu/anaconda/envs/disco/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 371, in forward
    output_parallel = self.quant_method.apply(self, input_, bias)
  File "/projectnb/replearn/mingyu/anaconda/envs/disco/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 135, in apply
    return F.linear(x, layer.weight, bias)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 236.00 MiB. GPU 0 has a total capacity of 44.40 GiB of which 22.31 MiB is free. Including non-PyTorch memory, this process has 44.37 GiB memory in use. Of the allocated memory 42.63 GiB is allocated by PyTorch, and 849.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

The above exception was the direct cause of the following exception:

[36mray::WorkerDict.actor_rollout_generate_sequences()[39m (pid=714520, ip=192.168.21.190, actor_id=b203ed3be4c45c7e69acd4da01000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x14b7e0078160>)
  File "/projectnb/rlhf/mingyuc/DisCO/verl/verl/workers/rollout/vllm_rollout/vllm_rollout.py", line 205, in generate_sequences
    output = self.inference_engine.generate(
  File "/projectnb/replearn/mingyu/anaconda/envs/disco/lib/python3.10/site-packages/vllm/utils.py", line 1063, in inner
    return fn(*args, **kwargs)
  File "/projectnb/replearn/mingyu/anaconda/envs/disco/lib/python3.10/site-packages/vllm/entrypoints/llm.py", line 353, in generate
    outputs = self._run_engine(use_tqdm=use_tqdm)
  File "/projectnb/rlhf/mingyuc/DisCO/verl/verl/third_party/vllm/vllm_v_0_6_3/llm.py", line 166, in _run_engine
    outputs = super()._run_engine(use_tqdm=use_tqdm)
  File "/projectnb/replearn/mingyu/anaconda/envs/disco/lib/python3.10/site-packages/vllm/entrypoints/llm.py", line 879, in _run_engine
    step_outputs = self.llm_engine.step()
  File "/projectnb/replearn/mingyu/anaconda/envs/disco/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 1386, in step
    outputs = self.model_executor.execute_model(
  File "/projectnb/rlhf/mingyuc/DisCO/verl/verl/third_party/vllm/vllm_v_0_6_3/spmd_gpu_executor.py", line 163, in execute_model
    all_outputs = self.worker.execute_model(execute_model_req=execute_model_req)
  File "/projectnb/rlhf/mingyuc/DisCO/verl/verl/third_party/vllm/vllm_v_0_6_3/worker.py", line 267, in execute_model
    return self.model_runner.execute_model(
  File "/projectnb/replearn/mingyu/anaconda/envs/disco/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/projectnb/replearn/mingyu/anaconda/envs/disco/lib/python3.10/site-packages/vllm/worker/model_runner_base.py", line 146, in _wrapper
    raise type(err)(f"Error in model execution: "
torch.OutOfMemoryError: Error in model execution: CUDA out of memory. Tried to allocate 236.00 MiB. GPU 0 has a total capacity of 44.40 GiB of which 22.31 MiB is free. Including non-PyTorch memory, this process has 44.37 GiB memory in use. Of the allocated memory 42.63 GiB is allocated by PyTorch, and 849.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

During handling of the above exception, another exception occurred:

[36mray::WorkerDict.actor_rollout_generate_sequences()[39m (pid=714520, ip=192.168.21.190, actor_id=b203ed3be4c45c7e69acd4da01000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x14b7e0078160>)
  File "/projectnb/rlhf/mingyuc/DisCO/verl/verl/single_controller/ray/base.py", line 399, in func
    return getattr(self.worker_dict[key], name)(*args, **kwargs)
  File "/projectnb/rlhf/mingyuc/DisCO/verl/verl/single_controller/base/decorator.py", line 404, in inner
    return func(*args, **kwargs)
  File "/projectnb/rlhf/mingyuc/DisCO/verl/verl/workers/fsdp_workers.py", line 445, in generate_sequences
    output = self.rollout.generate_sequences(prompts=prompts)
  File "/projectnb/replearn/mingyu/anaconda/envs/disco/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/projectnb/rlhf/mingyuc/DisCO/verl/verl/workers/rollout/vllm_rollout/vllm_rollout.py", line 280, in generate_sequences
    self.inference_engine = LLM(
  File "/projectnb/rlhf/mingyuc/DisCO/verl/verl/third_party/vllm/vllm_v_0_6_3/llm.py", line 147, in __init__
    self.llm_engine = LLMEngine.from_engine_args(model, tokenizer, engine_args)  # TODO: check usagecontext
  File "/projectnb/rlhf/mingyuc/DisCO/verl/verl/third_party/vllm/vllm_v_0_6_3/llm_engine_sp.py", line 393, in from_engine_args
    engine = cls(
  File "/projectnb/rlhf/mingyuc/DisCO/verl/verl/third_party/vllm/vllm_v_0_6_3/llm_engine_sp.py", line 212, in __init__
    self.model_executor = executor_class(
  File "/projectnb/rlhf/mingyuc/DisCO/verl/verl/third_party/vllm/vllm_v_0_6_3/spmd_gpu_executor.py", line 71, in __init__
    self._init_executor(model, distributed_init_method)
  File "/projectnb/rlhf/mingyuc/DisCO/verl/verl/third_party/vllm/vllm_v_0_6_3/spmd_gpu_executor.py", line 78, in _init_executor
    self._init_workers_sp(model, distributed_init_method)
  File "/projectnb/rlhf/mingyuc/DisCO/verl/verl/third_party/vllm/vllm_v_0_6_3/spmd_gpu_executor.py", line 112, in _init_workers_sp
    self.worker.load_model()
  File "/projectnb/replearn/mingyu/anaconda/envs/disco/lib/python3.10/site-packages/vllm/worker/worker.py", line 183, in load_model
    self.model_runner.load_model()
  File "/projectnb/rlhf/mingyuc/DisCO/verl/verl/third_party/vllm/vllm_v_0_6_3/model_runner.py", line 104, in load_model
    self.model = get_model(
  File "/projectnb/rlhf/mingyuc/DisCO/verl/verl/third_party/vllm/vllm_v_0_6_3/model_loader.py", line 45, in get_model
    return loader.load_model(
  File "/projectnb/rlhf/mingyuc/DisCO/verl/verl/third_party/vllm/vllm_v_0_6_3/model_loader.py", line 128, in load_model
    model = _initialize_model(model_config, self.load_config, lora_config, cache_config, scheduler_config)
  File "/projectnb/replearn/mingyu/anaconda/envs/disco/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 175, in _initialize_model
    return build_model(
  File "/projectnb/replearn/mingyu/anaconda/envs/disco/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 160, in build_model
    return model_class(config=hf_config,
  File "/projectnb/replearn/mingyu/anaconda/envs/disco/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py", line 393, in __init__
    self.model = Qwen2Model(config, cache_config, quant_config)
  File "/projectnb/replearn/mingyu/anaconda/envs/disco/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py", line 240, in __init__
    self.embed_tokens = VocabParallelEmbedding(
  File "/projectnb/replearn/mingyu/anaconda/envs/disco/lib/python3.10/site-packages/vllm/model_executor/layers/vocab_parallel_embedding.py", line 260, in __init__
    self.linear_method.create_weights(self,
  File "/projectnb/replearn/mingyu/anaconda/envs/disco/lib/python3.10/site-packages/vllm/model_executor/layers/vocab_parallel_embedding.py", line 28, in create_weights
    weight = Parameter(torch.empty(sum(output_partition_sizes),
  File "/projectnb/replearn/mingyu/anaconda/envs/disco/lib/python3.10/site-packages/torch/utils/_device.py", line 79, in __torch_function__
    return func(*args, **kwargs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 594.00 MiB. GPU 0 has a total capacity of 44.40 GiB of which 22.31 MiB is free. Including non-PyTorch memory, this process has 44.37 GiB memory in use. Of the allocated memory 42.63 GiB is allocated by PyTorch, and 849.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
